# GTPlanner: AI-Powered PRD Generation Tool

<p align="center">
  <img src="./assets/banner.png" width="800"/>
</p>

<p align="center">
  <strong>An intelligent Product Requirements Document (PRD) generation tool that transforms natural language descriptions into comprehensive technical documentation</strong>
</p>

<p align="center">
  <a href="#features">Features</a> •
  <a href="#installation">Installation</a> •
  <a href="#usage">Usage</a> •
  <a href="#api">API</a> •
  <a href="#architecture">Architecture</a> •
  <a href="#contributing">Contributing</a>
</p>

---

## 🎯 Overview

GTPlanner is an advanced AI-powered tool designed for "vibe coding" - transforming high-level ideas and requirements into detailed, structured technical documentation. Built on an asynchronous node-based architecture using [PocketFlow](https://github.com/The-Pocket/PocketFlow), it supports both interactive CLI usage and programmatic API access.

The project consists of two main components:
- **Backend Service**: Core processing engine with FastAPI REST API
- **MCP Service**: Model Context Protocol integration for seamless AI assistant integration

## ✨ Features

- **🗣️ Natural Language Processing**: Convert plain English requirements into structured PRDs
- **📝 Markdown Support**: Process and integrate existing Markdown documentation
- **⚡ Asynchronous Processing**: Full async pipeline for responsive performance
- **🔄 Multi-turn Optimization**: Interactive feedback loop for iterative document refinement
- **📊 Structured Output**: Generate standardized, customizable technical documentation
- **🧩 Extensible Architecture**: Modular node-based design for easy customization
- **🌐 Multiple Interfaces**: CLI, API, and MCP protocol support
- **🔧 LLM Agnostic**: Compatible with various language models via configurable endpoints

---

## Prerequisites

- **Python**: 3.10 or higher
- **Package Manager**: [uv](https://github.com/astral-sh/uv) (recommended) or pip
- **LLM API Access**: OpenAI-compatible API endpoint (OpenAI, Anthropic, local models, etc.)

## 🚀 Installation

### 1. Clone the Repository

```bash
git clone https://github.com/The-Agent-Builder/GTPlanner.git
cd GTPlanner
```

### 2. Install Dependencies

Using uv (recommended):
```bash
uv sync
```

Using pip:
```bash
pip install -r requirements.txt
```

### 3. Configuration

GTPlanner uses [Dynaconf](https://www.dynaconf.com/) for configuration management. Configure your LLM service by editing `settings.toml`:

```toml
[default.llm]
base_url = "https://api.openai.com/v1"  # Your LLM API endpoint
api_key = "your-api-key-here"           # Your API key
model = "gpt-4"                         # Model name
```

#### Environment Variables (Alternative)

You can also use environment variables:

```bash
export LLM_API_KEY="your-api-key-here"
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL="gpt-4"
```

This project supports OpenAI compatible llm providers.

---

## 🛠️ Usage

### 🧑‍💻 As a Backend API

Start the backend API service by running:

```
uv run fastapi_main.py
```

By default, the service is running on `http://0.0.0.0:11211`, you can go to `http://0.0.0.0:11211/docs` for documentations.


### As an MCP Server (Recommended)

**Note**: The MCP service has been refactored to directly call planning functions instead of depending on external API interfaces. No need to start the backend API separately.

1. Start MCP by running:

```bash
cd mcp
uv sync
uv run python mcp_service.py
```

The service is provided as `StreamableHTTP`.

2. To use the MCP service in any MCP-supported client, enter the following configuration:

```json
{
  "mcpServers": {
    "GT-planner": {
      "url": "http://127.0.0.1:8001/mcp"
    }
  }
}
```

### CLI Mode 

```bash
uv run python cli.py
```
- Enter your requirements as prompted; you can optionally attach Markdown files (main.py supports multiple files, cli.py supports only natural language)
- Supports multi-turn feedback and document optimization

---

## 🧩 System Architecture & Flow

This project adopts an asynchronous node flow (AsyncFlow) architecture, with the main nodes as follows:

1. **AsyncInputProcessingNode**: Handles user input (natural language/Markdown)
2. **AsyncRequirementsAnalysisNode**: Analyzes and extracts requirements
3. **AsyncDesignOptimizationNode**: Proposes design optimizations and synthesizes documentation
4. **AsyncDocumentationGenerationNode**: Generates the final technical documentation
5. **AsyncFeedbackProcessingNode**: Handles user feedback, supporting multi-turn optimization

The flowchart is as follows:

```mermaid
flowchart TD
    inputNode[Async Input Processing] --> analysisNode[Async Requirements Analysis]
    analysisNode --> optimizationNode[Async Design Optimization]
    optimizationNode --> docGenNode[Async Documentation Generation]
    docGenNode --> feedbackNode[Async Feedback Processing]
    feedbackNode -->|new_iteration| analysisNode
    feedbackNode -->|complete| endNode[End Process]
```

---

## 🧰 Main Utility Functions (`utils/`)
- `call_llm.py`: Async/sync LLM calls, supports JSON repair
- `parse_markdown.py`: Asynchronously parses Markdown, structurally extracts headings/paragraphs/code blocks, etc.
- `format_documentation.py`: Asynchronously formats output as standard technical documentation
- `store_conversation.py`: Asynchronously manages conversation history, supports save/load

---

## 📦 Project Structure
```
GTPlanner/
├── main.py                # Entry point, CLI and interactive logic
├── cli.py                 # CLI/interactive entry (simplified)
├── cli_flow.py            # CLI flow definition
├── filename_flow.py       # Auto-generate output file name
├── short_planner_flow.py  # Short flow generation and optimization
├── nodes.py               # Main async node implementations
├── utils/                 # Utility functions
│   ├── call_llm.py
│   ├── parse_markdown.py
│   ├── format_documentation.py
│   └── store_conversation.py
├── output/                # Output document directory
│   └── doc.md             # Example output
├── requirements.txt       # Dependency list
├── settings.toml          # Dynaconf configuration
├── pyproject.toml         # Build/metadata
├── fastapi_main.py        # FastAPI backend service (optional)
├── mcp/                   # MCP service related
│   └── mcp_service.py
├── api/                   # API implementation
│   └── v1/
│       └── planning.py
└── docs/                  # Design docs
    └── design.md
```

---

## 📝 Example:
```bash
uv run python main.py
```
After running, enter a one-sentence requirement description: "Youtube AI subtitle summarization"

- Output:
  - Structured technical documentation displayed in the terminal
  - Automatically saved to the `output/` directory (e.g., `output/doc.md`)

---

## 📄 Dependencies
- Python >= 3.10
- openai >= 1.0.0
- pyyaml >= 6.0
- pocketflow == 0.1.0
- dynaconf >= 3.1.12
- aiohttp >= 3.8.0
- json-repair >= 0.6.0



---

## 📚 Design & References
- Detailed design documentation: [docs/design.md](./docs/design.md)
- Reference project: [Pocket Flow](https://github.com/The-Pocket/PocketFlow)
- Configuration management: [Dynaconf](https://www.dynaconf.com/)

---

This project is an intelligent requirements analysis and documentation generation engine based on [Pocket Flow](https://github.com/The-Pocket/PocketFlow), supporting natural language and Markdown input, asynchronous multi-turn optimization, and automatic generation of structured technical documentation.

## License

MIT